"""
search.py (FINAL STABLE)
--------------------------------------------------
âœ… FAISS index preload (startup)
âœ… SigLIP embedding ì‚¬ìš©
âœ… ê·¸ë£¹ í‰ê·  ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰
âœ… cosine similarity ê¸°ì¤€
âœ… threshold ê¸°ë°˜ ë§¤ì¹­ íŒì •
âœ… Node ì„œë²„ ì—°ë™ìš© ì•ˆì • ì‘ë‹µ êµ¬ì¡°
âœ… ğŸ”¥ group-search ìƒì„¸ ë¡œê·¸ ì¶”ê°€ (í›„ë³´ / score / íŒì •)
âœ… ğŸ”¥ (ì¶”ê°€) group-search ë¯¸ì„¸ ìµœì í™” + íƒ€ì´ë° ë¡œê·¸
"""

import os
import logging
import time
import numpy as np
import faiss
from PIL import Image

from siglip import image_to_vector


# ==================================================
# Logger
# ==================================================
logger = logging.getLogger(__name__)


# ==================================================
# Path Config
# ==================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

INDEX_PATH = os.path.join(BASE_DIR, "index", "siglip.index")
IDS_PATH = os.path.join(BASE_DIR, "index", "product_ids.npy")


# ==================================================
# Search Config (ê¸°ì¡´ ìœ ì§€)
# ==================================================
SIMILARITY_THRESHOLD = 0.3
TOP_K = 5


# ==================================================
# Threshold Config (ğŸ”¥ ìë™ íŠœë‹)
# ==================================================
MIN_THRESHOLD = 0.45
GAP_THRESHOLD = 0.07


# ==================================================
# Load Assets (1íšŒ)
# ==================================================
def _load_assets():
    if not os.path.exists(INDEX_PATH):
        raise FileNotFoundError(f"FAISS index not found: {INDEX_PATH}")

    if not os.path.exists(IDS_PATH):
        raise FileNotFoundError(f"product_ids not found: {IDS_PATH}")

    index = faiss.read_index(INDEX_PATH)
    product_ids = np.load(IDS_PATH, allow_pickle=True)

    if index.ntotal != len(product_ids):
        raise RuntimeError("Index size and product_ids length mismatch")

    logger.info("[FAISS] index loaded (total_groups=%d)", index.ntotal)
    return index, product_ids


INDEX, PRODUCT_IDS = _load_assets()


# ==================================================
# ì „ì²´ DB ê²€ìƒ‰ (ê¸°ì¡´ ìœ ì§€ âŒ ë³€ê²½ ê¸ˆì§€)
# ==================================================
def search_image(image_path: str, top_k: int = TOP_K):
    img = Image.open(image_path).convert("RGB")
    q = image_to_vector(img).reshape(1, -1)

    sims, idxs = INDEX.search(q, top_k)

    results = []
    for sim, idx in zip(sims[0], idxs[0]):
        if idx < 0:
            continue

        pid = PRODUCT_IDS[int(idx)]
        similarity = float(sim)
        distance = float(1.0 - similarity)

        results.append({
            "product_id": str(pid),
            "similarity": similarity,
            "distance": distance,
        })

    if not results:
        return {"matched": False, "best": None, "results": []}

    best = results[0]

    if best["similarity"] < SIMILARITY_THRESHOLD:
        return {"matched": False, "best": best, "results": results}

    return {"matched": True, "best": best, "results": results}


# ==================================================
# ë‚´ë¶€ ìœ í‹¸ (ë¯¸ì„¸ ìµœì í™”)
# ==================================================
def _l2_normalize(v: np.ndarray) -> np.ndarray:
    # np.linalg.normì€ ë‚´ë¶€ì ìœ¼ë¡œ sqrt(dot)ì¸ë°,
    # float32 ê¸°ì¤€ìœ¼ë¡œ ì•„ë˜ê°€ ë¯¸ì„¸í•˜ê²Œ ë” ë¹ ë¥¸ ê²½ìš°ê°€ ë§ìŒ.
    denom = float(np.sqrt(np.dot(v, v)))  # type: ignore
    if denom == 0.0:
        return v
    return v / denom


def _embed_image_path(img_path: str) -> np.ndarray:
    """
    - PIL open/convert ì•ˆì •í™”
    - image_to_vector í˜¸ì¶œ í›„ float32 ì •ê·œí™”
    """
    with Image.open(img_path) as im:
        im = im.convert("RGB")
        v = image_to_vector(im).astype("float32")
    return _l2_normalize(v)


# ==================================================
# ğŸ”¥ ì‚¬ìš©ì íŒŒìš°ì¹˜ ê·¸ë£¹ ê²€ìƒ‰ (Node ì—°ë™ìš©)
# ==================================================
def search_image_with_groups(image_path: str, groups: dict):
    """
    image_path: ì´¬ì˜ ì´ë¯¸ì§€ ê²½ë¡œ
    groups: {
        "12": ["/tmp/12/img1.jpg", "/tmp/12/img2.jpg", "/tmp/12/img3.jpg", "/tmp/12/img4.jpg"],
        ...
    }

    return:
    {
        "matched": bool,
        "group_id": str | None,
        "score": float
    }
    """

    t0 = time.perf_counter()

    logger.info(
        "[GROUP_SEARCH][START] groups=%d image=%s",
        len(groups),
        os.path.basename(image_path),
    )

    if not groups:
        logger.info("[GROUP_SEARCH][RESULT] empty groups")
        return {"matched": False, "group_id": None, "score": -1.0}

    # --------------------------------------------------
    # 1ï¸âƒ£ Query embedding (1íšŒ)
    # --------------------------------------------------
    tq0 = time.perf_counter()
    q = _embed_image_path(image_path)
    tq1 = time.perf_counter()
    logger.info("[GROUP_SEARCH][TIME] query_embed_ms=%.1f", (tq1 - tq0) * 1000)

    # --------------------------------------------------
    # 2ï¸âƒ£ ê·¸ë£¹ë³„ 1:4 ë¹„êµ (max ê¸°ì¤€)
    # --------------------------------------------------
    group_scores = []
    embed_count = 0
    failed_count = 0

    for group_id, image_paths in groups.items():
        if not image_paths:
            continue

        scores = []

        # group ë‹¨ìœ„ íƒ€ì´ë°(ì›ì¸ ì¶”ì ìš©)
        tg0 = time.perf_counter()

        for img_path in image_paths:
            try:
                v = _embed_image_path(img_path)
                embed_count += 1

                sim = float(np.dot(q, v))
                scores.append(sim)

                logger.debug(
                    "[GROUP_SEARCH][SCORE] group=%s img=%s sim=%.4f",
                    group_id,
                    os.path.basename(img_path),
                    sim,
                )

            except Exception as e:
                failed_count += 1
                logger.warning(
                    "[GROUP_SEARCH][IMAGE_FAIL] group=%s img=%s err=%s",
                    group_id,
                    img_path,
                    str(e),
                )

        if not scores:
            continue

        max_score = max(scores)
        avg_score = sum(scores) / len(scores)

        tg1 = time.perf_counter()
        logger.info(
            "[GROUP_SEARCH][GROUP_SUMMARY] group=%s max=%.4f avg=%.4f imgs=%d ms=%.1f",
            group_id,
            max_score,
            avg_score,
            len(scores),
            (tg1 - tg0) * 1000,
        )

        group_scores.append({"group_id": str(group_id), "max": float(max_score)})

    if not group_scores:
        logger.info(
            "[GROUP_SEARCH][RESULT] no valid groups embed_count=%d failed=%d",
            embed_count,
            failed_count,
        )
        return {"matched": False, "group_id": None, "score": -1.0}

    # --------------------------------------------------
    # 3ï¸âƒ£ ìë™ íŠœë‹ íŒì • (min + gap)
    # --------------------------------------------------
    group_scores.sort(key=lambda x: x["max"], reverse=True)

    best = group_scores[0]
    second = group_scores[1] if len(group_scores) > 1 else None

    best_score = best["max"]
    second_score = second["max"] if second else 0.0
    gap = best_score - second_score

    matched = (best_score >= MIN_THRESHOLD) and (gap >= GAP_THRESHOLD)

    t1 = time.perf_counter()
    logger.info(
        "[GROUP_SEARCH][DECISION] matched=%s best=%s score=%.4f second=%.4f gap=%.4f "
        "min_th=%.2f gap_th=%.2f total_ms=%.1f embed_count=%d failed=%d",
        matched,
        best["group_id"],
        best_score,
        second_score,
        gap,
        MIN_THRESHOLD,
        GAP_THRESHOLD,
        (t1 - t0) * 1000,
        embed_count,
        failed_count,
    )

    return {
        "matched": matched,
        "group_id": best["group_id"] if matched else None,
        "score": float(best_score),
    }
