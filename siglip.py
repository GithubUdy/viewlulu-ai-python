"""
siglip.py (EC2 ì•ˆì •í™” ìµœì¢…ë³¸)
--------------------------------------------------
âœ… ì„œë²„ ì‹œìž‘ ì‹œ ëª¨ë¸ 1íšŒ ë¡œë”©
âœ… ìš”ì²­ ì‹œ ìž¬ë¡œë”© ì—†ìŒ
âœ… CPU ê°•ì œ ì‚¬ìš©
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”
"""

import torch
import numpy as np
from PIL import Image
import open_clip

# ==============================
# í™˜ê²½ ê³ ì •
# ==============================
DEVICE = "cpu"  # EC2ì—ì„œëŠ” ë¬´ì¡°ê±´ CPU
MODEL_NAME = "ViT-B-16-SigLIP-384"

# ==============================
# ëª¨ë¸ 1íšŒ ë¡œë”© (ì„œë²„ ì‹œìž‘ ì‹œ)
# ==============================
print("ðŸ”¥ Loading SigLIP model (one-time)...")

model, _, preprocess = open_clip.create_model_and_transforms(
    MODEL_NAME,
    pretrained="webli"
)

model = model.to(DEVICE)
model.eval()

print("âœ… SigLIP model loaded and ready")

# ==============================
# ì´ë¯¸ì§€ â†’ ë²¡í„° ë³€í™˜
# ==============================
def image_to_vector(img: Image.Image) -> np.ndarray:
    """
    PIL Image â†’ normalized embedding vector (float32)
    """
    img = img.convert("RGB")
    img_t = preprocess(img).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        feat = model.encode_image(img_t)
        feat = feat / feat.norm(dim=-1, keepdim=True)

    return feat.squeeze(0).cpu().numpy().astype("float32")
